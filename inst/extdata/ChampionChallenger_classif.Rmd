---
title: "ChampionChallenger"
author: "DALEXtra"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true  
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE, message=FALSE, warning=TRUE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


```{r}
library("auditor")
```

# Measures 

```{r}
confusionmatrix <- function(explainer) {
  yhat <- as.numeric(explainer$y_hat > 0.5)
  TP <- sum(yhat[yhat == 1] == explainer$y[yhat == 1])
  FP <- length(yhat[yhat == 1]) - TP
  TN <- sum(yhat[yhat == 0] == explainer$y[yhat == 0])
  FN <- length(yhat[yhat == 0]) - TN
  list(
    "TP" = TP,
    "FP" = FP,
    "TN" = TN,
    "FN" = FN
  )
}

new_scores <- list(
  "1-auc" = function(au) {
    1 - score(au, score = "auc")$score
  },
  "1-acc" = function(au) {
    conf <- confusionmatrix(au)
    1 - (conf$TP + conf$TN) / (conf$TP + conf$FP + conf$TN + conf$FN)
  },
  "1-precission" = function(au) {
    conf <- confusionmatrix(au)
    1 - conf$TP / (conf$TP + conf$FP)
  },
  "1-recall" = function(au) {
    conf <- confusionmatrix(au)
    1 - conf$TP / (conf$TP + conf$FN)
  },
  "1-specificity" = function(au) {
    conf <- confusionmatrix(au)
    1 - conf$TN / (conf$TN + conf$FP)
  },
  "1-F1" = function(au) {
    conf <- confusionmatrix(au)
    (2 * (conf$TP / (conf$TP + conf$FP)) * (conf$TP / (conf$TP + conf$FN))) /
      (conf$TP / (conf$TP + conf$FN) + conf$TP / (conf$TP + conf$FP))
  }
)
radar_args <- lapply(challenger, auditor::model_performance, score = NULL, new_score = new_scores)
radar_args$object <- auditor::model_performance(champion, score = NULL, new_score = new_scores)
radar_args$print = FALSE
p <- do.call(plot_radar, args = radar_args)
p

p$data$`_score_` <- round(p$data$`_score_`, 4)

DT::datatable(p$data, class = 'cell-border stripe')

```

# Scores comparison

Sometimes mesuares or even graphics that are presented below, are not sufficient to judge which model is better. Therefore here we have some scores. DW score and Runs score are based on Durbin-Watson and Runs test statistics.

```{r}
scores <- data.frame(round(c(score_dw(champion)$score, score_runs(champion)$score), 4))
for (i in challenger) {
scores <- data.frame(scores, round(c(score_dw(i)$score, score_runs(i)$score), 4))  
}
exp_labels <- sapply(challenger, function(x){
  x$label
})
colnames(scores) <- c(champion$label, exp_labels)
rownames(scores) <- c("Durbin-Watson", "Runs")
DT::datatable(scores, class = 'cell-border stripe')
```

# Response comparison

It is important to get to know where models does not agree with eachother. Thats why, decision tree with response from models as y column is being train for champion and each challenger, to discover partitions of dataset. Afterwards, we check partition by partition which area is the most controversial and present it in the table.

```{r}
library(rpart)
testdata_champion <-
  cbind(champion$data,
        "survived" = as.numeric(champion$y_hat > 0.5))
testdata_challenger <- lapply(challenger, function(x){
    cbind(x$data,
        "survived" = as.numeric(x$y_hat > 0.5))
})

yhats <- data.frame(
  as.numeric(champion$y_hat > 0.5)
)
for (i in challenger) {
  yhats <- data.frame(yhats, as.numeric(i$y_hat > 0.5))
}
colnames(yhats) <- c(champion$label, exp_labels)

model_champion <-
  rpart(survived ~ .,
        data = testdata_champion,
        control = control)

model_challenger <- lapply(testdata_challenger, function(x){
    rpart(survived ~ .,
        data = x,
        control = control)
})


datasets_champion <- lapply(sort(unique(model_champion$where)), function(x){
  yhats[model_champion$where == x,]
})
names_champion <- row.names(model_champion$frame[model_champion$frame$var == "<leaf>",])
rules_champion <- rpart.plot::rpart.rules(model_champion, roundint = FALSE) 
rules_champion  <- rules_champion[names_champion,]
datasets_challenger <- lapply(model_challenger, function(M) {
  dataset_challenger <- lapply(sort(unique(M$where)), function(x) {
    yhats[M$where == x, ]
  })
  names_challenger <- row.names(M$frame[M$frame$var == "<leaf>", ])
  rules_challenger <- rpart.plot::rpart.rules(M, roundint = FALSE)
  rules_challenger <- rules_challenger[names_challenger, ]
  list("data" = dataset_challenger, "rules" = rules_challenger)
})

entropy <- function(x) {
  if (sum(0 == x) == 0) {
    ret_0 <- 0
  } else {
    ret_0 <- (sum(0 == x) / length(x)) * log2(sum(0 == x) / length(x))
  }
  if (sum(1 == x) == 0) {
    ret_1 <- 0
  } else {
    ret_1 <- (sum(1 == x) / length(x)) * log2(sum(1 == x) / length(x))
  }


  - ret_0 - ret_1
}

loss_function <- function(X){
  rows <- sum(apply(X, MARGIN = 1, entropy))
  columns <- sum(apply(X, MARGIN = 2, entropy))
  rows/nrow(X) - columns/ncol(X)
}

scores_champion <- sapply(datasets_champion, loss_function)
scores_challenger <- lapply(datasets_challenger, function(x){
  sapply(x$data, loss_function)
})

max_challenger_index <- which.max(sapply(scores_challenger, max))
max_challenger <- challenger[[max_challenger_index]]
max_scores_challenger <- scores_challenger[[max_challenger_index]]
max_datasets_challenger <- datasets_challenger[[max_challenger_index]]$data
max_rules_challenger <- datasets_challenger[[max_challenger_index]]$rules
    

if (max(scores_champion) >= max(max_scores_challenger)) {
  controversial_data <-
    cbind(champion$data,
          "target" = max_challenger$y,
          "response" = champion$y_hat)[row.names(datasets_champion[[which.max(scores_champion)]]),]
  controversial_data_rule <-
    paste0(rules_champion[which.max(scores_champion), -(1:2)], collapse = " ")
  plot_data <- lapply(challenger, function(x) {
    cbind(champion$data,
          "target" = max_challenger$y,
          "response" = x$y_hat)[row.names(datasets_champion[[which.max(scores_champion)]]),]
  })
} else{
  controversial_data <-
    cbind(max_challenger$data,
          "target" = max_challenger$y,
          "response" = max_challenger$y_hat)[row.names(max_datasets_challenger[[which.max(max_scores_challenger)]]),]
  controversial_data_rule <-
    paste0(max_rules_challenger[which.max(max_scores_challenger), -(1:2)], collapse = " ")
  plot_data <-
    list(
      cbind(
        max_challenger$data,
        "target" = max_challenger$y,
        "response" = champion$y_hat
      )[row.names(max_datasets_challenger[[which.max(max_scores_challenger)]]),]
    )
  for (i in 1:length(challenger)) {
    if (i == max_challenger_index) {
      next()
    }
    plot_data[[length(plot_data) + 1]] <-
      cbind(max_challenger$data,
            "target" = max_challenger$y,
            "response" = challenger[[i]]$y_hat)[row.names(max_datasets_challenger[[which.max(max_scores_challenger)]]), ]
  }
}




DT::datatable(controversial_data, class = 'cell-border stripe', caption = controversial_data_rule, options = list(scrollX='400px'))
acc_table <- yhats[as.numeric(row.names(controversial_data)),]
acc_table <- lapply(acc_table, function(x){
  round(sum(x == champion$y[as.numeric(row.names(controversial_data))])/length(x), 4)
})
DT::datatable(data.frame(acc_table), class = 'cell-border stripe', caption = "accuracy comparison")
accordance_table <- yhats[as.numeric(row.names(controversial_data)),]
accordance_table <- lapply(accordance_table[,-1], function(x){
  round(sum(x == accordance_table[,1])/length(x), 4)
})
DT::datatable(data.frame(accordance_table), class = 'cell-border stripe', caption = "accordance with champion")

```


# Residual comparison

## Autocorrealation of residuals

```{r}
champion_model_residual <- model_residual(champion)
challenger_model_residual <- lapply(challenger, model_residual)
plot_params <- challenger_model_residual
plot_params$object <- champion_model_residual
do.call(plot_autocorrelation, args = plot_params)
```

## Receiver Operating Characteristic (ROC) Curve

```{r}
champion_model_evaluation <- model_evaluation(champion)
challenger_model_evaluation <- lapply(challenger, model_evaluation)
plot_params <- challenger_model_evaluation
plot_params$object <- champion_model_evaluation
do.call(plot_roc, args = plot_params)
```

## Prediction vs variable

```{r}
library(ggplot2)
if(is.null(variable)) {
  plot_params <- challenger_model_residual
  plot_params$object <- champion_model_residual
  p <- do.call(plot_prediction, plot_params)
  p <- p + geom_point(data = controversial_data, aes(x = controversial_data$target, y = controversial_data$response))
  for (i in plot_data) {
    p <- p + geom_point(data = i, aes(x = i$target, y = i$response))
  }
  print(p)
} else {
  for (var in variable) {
  plot_params <- challenger_model_residual
  plot_params$object <- champion_model_residual
  plot_params$variable <- var
  p <- do.call(plot_prediction, plot_params)
  p <- p + geom_point(data = controversial_data, aes(x = controversial_data[,var], y = controversial_data$response))
  for (i in plot_data) {
    p <- p + geom_point(data = i, aes(x = i[,var], y = i$response))
  }
  print(p)
  }
}

```

## ResidualBoxplot

```{r}
plot_params <- challenger_model_residual
plot_params$object <- champion_model_residual
do.call(plot_residual_boxplot, plot_params)
```

## Residual Density

```{r}
plot_params <- challenger_model_residual
plot_params$object <- champion_model_residual
do.call(plot_residual_density, plot_params)

```

## LIFT curve

```{r}
plot_params <- challenger_model_evaluation
plot_params$object <- champion_model_evaluation
do.call(plot_lift, args = plot_params)
```

