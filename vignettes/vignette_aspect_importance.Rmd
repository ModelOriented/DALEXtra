---
title: "Aspect importance function examples"
author: "Katarzyna PÄ™kala"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Aspect importance function examples}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```


# Data and logistic regression model for Titanic survival

Vignette presents the aspect_importance() function on the datasets: `titanic_imputed` and `apartments` (both are available in the `DALEX` package). 
At the beginning, we download `titanic_imputed` dataset and build logistic regression model.

```{r read titanic data}
library("DALEX")
titanic <- titanic_imputed
titanic$country <- NULL
titanic_without_target <- titanic[,colnames(titanic)!="survived"]
head(titanic)

model_titanic_glm <-
  glm(survived == "yes" ~ class + gender + age + sibsp + parch + fare + embarked,
      titanic,
      family = "binomial")

```

#  Preparing additional parameters

Before using aspect_importance() we need to:

* group features of the dataset into aspects, 
* choose observation for which we want to explain aspects' importance.

```{r build parameters for titanic model}
aspects_titanic <-
  list(
    wealth = c("class", "fare"),
    family = c("sibsp", "parch"),
    personal = c("age", "gender"),
    embarked = "embarked"
  )
passenger <- data.frame(
  class = factor(
    "3rd",
    levels = c("1st", "2nd", "3rd", "deck crew", "engineering crew", "restaurant staff", "victualling crew")),
  gender = factor("male", levels = c("female", "male")),
  age = 8,
  sibsp = 0,
  parch = 0,
  fare = 18,
  embarked = factor(
    "Southampton",
    levels = c("Belfast", "Cherbourg", "Queenstown", "Southampton")
  )
)

passenger
predict(model_titanic_glm, passenger, type = "response")

```

# Calculating aspect importance (logistic regression)

Now we can call aspect_importance() function and see that features included in `wealth` (that is `class` and `fare`) have the biggest contribution on survival prediction for the passenger. That contribution is of negative type. Rest of the aspects have significantly smaller influence. However, in case of `family` and `personal`, it's a positive type of influence.

```{r Calculating aspect importance (logistic regression)}
library("ggplot2")
library("ingredients")

titanic_glm_ai <- aspect_importance(model_titanic_glm, titanic, predict, passenger, aspects_titanic, N = 1000, label = "logistic regression")

titanic_glm_ai
plot(titanic_glm_ai) + ggtitle("Aspect importance for the selected passenger")
```


# Calculating aspect importance with explainer

Aspect_importance() could be also called using `DALEX` explainer as we show below.

```{r Calculating aspect importance with explainer}
explain_titanic_glm <- explain(model_titanic_glm, 
                      data = titanic_without_target,
                      y = titanic$survived == "yes", 
                      predict_function = predict,
                      label = "Logistic Regression", 
                      verbose = FALSE)

titanic_glm_ai <- aspect_importance(explain_titanic_glm, passenger, aspects_titanic, N = 1000)
titanic_glm_ai

```

# Random forest model for Titanic survival

Now, we prepare random forest model for the `titanic` dataset.

```{r  Random forest model for Titanic survival}
library("randomForest")
model_titanic_rf <- randomForest(factor(survived) == "yes" ~ gender + age + class + embarked + fare + sibsp + parch,  data = titanic)
predict(model_titanic_rf, passenger)
```

# Calculating aspect importance (random forest)

After calling aspect_importance() we can see why the survival prediction for the passenger in random forest model was much higher (0.5) than in logistic regression case (0.18). 

In this example `personal` features (`age` and `gender`) have the biggest positive influence. Aspects `wealth` (`class`, `fare`) and `embarked` have both much smaller contribution and those are negative ones. Aspect `family` has very small influence on the prediction.

```{r Calculating aspect importance (random forest)}
titanic_rf_ai <- aspect_importance(model_titanic_rf, titanic, predict, passenger, aspects_titanic, N = 1000, label = "random forest")

titanic_rf_ai
plot(titanic_rf_ai) + ggtitle("Aspect importance for the selected passenger")
```

# Using lasso in aspect_importance() function

Function aspect_importance() can calculate coefficients (that is aspects' importance) by using either linear regression or lasso regression. Using lasso, we can control how many nonzero coefficients (nonzero aspects importance values) are present in the final explanation.

To use aspect_importance() with lasso, we have to provide `n_var` parameter, which declares how many aspects importance values we would like to get in aspect_importance() results.

For this example, we use `titanic_imputed` dataset again and random forest model. With the help of lasso technique, we would like to check the importance of variables' aspects, while controlling that one of them should be equal to 0. Therefore we call aspect_importance() with `n_var` parameter set to 3. 

```{r lasso demo}
titanic_rf_ai_lasso <- aspect_importance(model_titanic_rf, titanic, predict, passenger, aspects_titanic, N = 1000, n_var = 3)
titanic_rf_ai_lasso
```


# Automated grouping features into aspects

In examples described above, we had to manually group features into aspects. 
On `apartments` dataset, we will test the function that automatically groups features for us (grouping is based on the features correlation). Function only works on numeric variables.  

We import `apartments` from `DALEX` package and choose columns with numeric features. Then we fit linear model to the data and choose observation to be explained. Target variable is `m2.price`. 


```{r import apartments}

library(DALEX)
data("apartments")
apartments_num <- apartments[,unlist(lapply(apartments, is.numeric))] #excluding non numeric features
head(apartments_num)
apartments_no_target <- apartments_num[,-1] #excluding target variable
new_observation_apartments <- apartments_num[1,]
model_apartments <- lm(m2.price ~ ., data = apartments_num)

```

We run group_variables() function with cut off level set on 0.6. As a result, we get a list of variables groups (aspects) where absolute value of features' pairwise correlation is at least at 0.6.

Afterwards, we call aspect_importance() function with parameter `show_cor = TRUE`, to check how features are grouped into aspects, what is minimal value of pairwise correlation in each group and to check whether any pair of features is negatively correlated (`neg`) or not (`pos`). 

```{r}
aspects_apartments <- group_variables(apartments_no_target, 0.6)
apartments_ai <- aspect_importance(x = model_apartments, data = apartments_no_target, new_observation = new_observation_apartments, aspects = aspects_apartments, N = 500, show_cor = TRUE)
apartments_ai
```

# Hierarchical aspects importance

`Triplot` is one more tool that allows us to better understand the inner workings a of black box model. It illustrates, in one place: 

* the importance of every single feature,
* hierarchical aspects importance (explained below), 
* order of grouping features into aspects in `group_variables()`.

Hierarchical aspects importance allows us to check the values of aspects importance for the different levels of variables grouping. Method starts with looking at the aspect importance where every aspect has one, single variable. Afterwards, it iteratively creates bigger aspects by merging the ones with the highest level of absolute correlation into one aspect and calculating it's contribution to the prediction. 

It should be noted that similarly to `group_variables()`, `triplot()` works for the datasets with only numerical variables. 

```{r}
triplot(model_apartments, apartments_no_target, 
        new_observation = new_observation_apartments, N = 500, 
        clust_method = "complete", absolute_value = FALSE, 
        cumulative_max = FALSE, add_importance_labels = FALSE, 
        abbrev_labels = 15)
```


# Session info

```{r}
sessionInfo()
```

